---
title: "PyTorch源码解析系列 - 第12章：CUDA算子开发基础"
collection: pytorch
permalink: /pytorch/2025-12-05-pytorch-12
excerpt: "本章深入讲解PyTorch中CUDA算子的开发流程和优化技巧，涵盖从基础的CUDA编程模型到PyTorch集成的各个方面。"
date: 2025-12-05
---

## 1 概述

CUDA算子是PyTorch GPU加速的核心，掌握CUDA编程能够实现高性能的自定义操作。本章将深入讲解PyTorch中CUDA算子的开发流程和优化技巧。

### 1.1 为什么需要自定义CUDA算子

**PyTorch内置算子的局限性**：
- 特定领域算法未内置（如自定义注意力机制）
- 性能瓶颈需要专门优化
- 需要融合多个操作以减少内存传输
- 研究新型算子架构

**自定义CUDA算子的优势**：
- 直接访问GPU硬件能力
- 精细控制内存访问模式
- 实现专用优化（如利用Tensor Core）
- 性能提升可达10-100倍

### 1.2 CUDA编程模型回顾

```mermaid
graph TB
    subgraph Host主机
        A[CPU代码] --> B[分配GPU内存]
        B --> C[拷贝数据到GPU]
        C --> D[启动Kernel]
        D --> E[拷贝结果回CPU]
        E --> F[释放GPU内存]
    end
    
    subgraph Device设备
        G[Grid网格] --> H[Block 0]
        G --> I[Block 1]
        G --> J[Block N]
        
        H --> K[Thread 0]
        H --> L[Thread 1]
        H --> M[Thread M]
    end
    
    D --> G
    
    style A fill:#e1f5ff
    style D fill:#ffe1f5
    style G fill:#fff4e1
    style K fill:#e1ffe1
```

**核心概念**：
- **Grid（网格）**：整个kernel的执行空间
- **Block（块）**：线程组，共享内存和同步
- **Thread（线程）**：最小执行单元
- **Warp（束）**：32个线程的SIMD组

### 1.3 GPU 调度深入解析

#### 1.3.1 显卡、架构与计算能力

**什么是Compute Capability（计算能力）？**

计算能力是NVIDIA用来标识GPU硬件特性的版本号，格式为`X.Y`：
- **主版本号（X）**：代表GPU的核心架构
- **次版本号（Y）**：代表该架构的改进版本


**为什么编译时需要指定架构？**

CUDA代码需要编译成两部分：
1. **PTX（中间代码）**：类似于字节码，可以在运行时JIT编译
2. **SASS（机器码）**：针对特定GPU的二进制代码

```bash
# 为Ampere架构（计算能力8.6）编译
nvcc -arch=sm_86 kernel.cu -o kernel

# 同时生成多个架构的代码（胖二进制）
nvcc -gencode arch=compute_75,code=sm_75 \
     -gencode arch=compute_80,code=sm_80 \
     -gencode arch=compute_86,code=sm_86 \
     kernel.cu -o kernel

# PyTorch中查看计算能力
# Python代码：
# torch.cuda.get_device_capability()  # 返回 (major, minor)
```

**常见编译选项说明**：

```bash
# -arch=sm_XX: 指定目标架构
# sm = streaming multiprocessor（流式多处理器）
nvcc -arch=sm_86  # 为计算能力8.6编译

# -code=sm_XX: 生成二进制代码
# -code=compute_XX: 生成PTX代码
nvcc -gencode arch=compute_86,code=sm_86      # 生成SASS
nvcc -gencode arch=compute_86,code=compute_86 # 生成PTX

# --use_fast_math: 使用快速数学库（可能损失精度）
nvcc --use_fast_math kernel.cu

# -G: 生成调试信息
nvcc -G -g kernel.cu

# -lineinfo: 生成行号信息（用于profiler）
nvcc -lineinfo kernel.cu
```

#### 1.3.2 GPU硬件架构详解

**SM（Streaming Multiprocessor）是什么？**

SM是GPU的核心计算单元，可以理解为一个"小CPU"，包含：
- **CUDA Core**：执行单精度浮点运算
- **Tensor Core**（新架构）：专门用于矩阵运算
- **Special Function Units (SFU)**：执行超越函数（sin, cos, sqrt等）
- **Load/Store Units**：处理内存访问
- **Warp Scheduler**：调度warp执行
- **Shared Memory / L1 Cache**：快速片上内存

**示例：A100 GPU架构**

```mermaid
graph TB
    subgraph A100 GPU
        GPC1[GPC 0<br/>图形处理集群]
        GPC2[GPC 1]
        GPC3[GPC 7]
        
        GPC1 --> SM1[SM 0]
        GPC1 --> SM2[SM 1]
        GPC1 --> SM3[SM 13]
        
        SM1 --> Core1[64个FP32 CUDA Core]
        SM1 --> Core2[32个FP64 CUDA Core]
        SM1 --> TC[4个Tensor Core]
        SM1 --> SFU[16个SFU]
        SM1 --> LS[32个LD/ST单元]
        SM1 --> Sched[4个Warp调度器]
        SM1 --> Smem[192KB 共享内存]
        
        L2[40MB L2 Cache]
        HBM[40GB HBM2内存]
        
        SM1 --> L2
        SM2 --> L2
        SM3 --> L2
        L2 --> HBM
    end
    
    style A100 fill:#e1f5ff
    style SM1 fill:#fff4e1
    style Core1 fill:#e1ffe1
    style TC fill:#ffe1f5
```

#### 1.3.3 完整的GPU调度流程

```mermaid
graph TB
    subgraph CPU端
        A[Python/C++代码] --> B[准备Kernel参数]
        B --> C[计算Grid和Block维度]
        C --> D[发起Kernel Launch]
    end
    
    subgraph GPU调度器
        D --> E[Kernel进入全局队列]
        E --> F[Block分配器]
        F --> G{有空闲SM?}
        G -->|是| H[分配Block到SM]
        G -->|否| I[等待SM空闲]
        I --> G
    end
    
    subgraph SM内部调度
        H --> J[Block分解为Warp]
        J --> K[Warp进入SM队列]
        K --> L[Warp调度器选择就绪Warp]
        L --> M[Warp分配到CUDA Core]
        M --> N[32个Thread并行执行]
        N --> O{Warp完成?}
        O -->|否| K
        O -->|是| P{Block完成?}
        P -->|否| K
        P -->|是| Q[释放SM资源]
    end
    
    subgraph 执行完成
        Q --> R{所有Block完成?}
        R -->|否| G
        R -->|是| S[Kernel执行完成]
        S --> T[返回CPU]
    end
    
    style A fill:#e1f5ff
    style D fill:#fff4e1
    style H fill:#ffe1f5
    style N fill:#e1ffe1
    style S fill:#90EE90
```

**详细调度步骤解析**：

**第1步：CPU端Kernel启动**

```cpp
// 在CPU代码中启动kernel
dim3 grid(256);      // 256个block
dim3 block(256);     // 每个block有256个thread

my_kernel<<<grid, block>>>(d_input, d_output, n);
// ^^^^ 三尖括号语法是CUDA特有的kernel启动方式
```

这一步会：
1. 将kernel参数打包
2. 将启动配置发送到GPU
3. CPU继续执行（异步启动）

**第2步：Block进入全局队列**

```
所有Block: [Block_0] [Block_1] [Block_2] ... [Block_255]
                ↓
           全局Block队列
```

- 所有256个Block进入GPU的调度队列
- **Block之间没有执行顺序保证**（可能乱序执行）
- Block是**独立的**，不能相互通信

**第3步：Block分配到SM**

假设GPU有80个SM：

```
SM_0:  [Block_0] [Block_80]  [Block_160]
SM_1:  [Block_1] [Block_81]  [Block_161]
SM_2:  [Block_2] [Block_82]  [Block_162]
...
SM_79: [Block_79] [Block_159] [Block_239]
```

关键点：
- **一个SM可以同时运行多个Block**（取决于资源限制）
- **一个Block只能在一个SM上运行**（不能跨SM）
- Block分配完全由硬件调度器决定

**资源限制示例（A100）**：

每个SM的资源限制：
- 最大Block数：32个
- 最大Thread数：2048个
- 最大Warp数：64个
- 共享内存：164KB

如果你的每个Block使用：
- 256个thread（8个warp）
- 16KB共享内存

那么一个SM最多可以同时运行：
```
min(
    32 blocks,                    # Block数限制
    2048 / 256 = 8 blocks,       # Thread数限制
    64 / 8 = 8 blocks,           # Warp数限制
    164KB / 16KB = 10 blocks     # 共享内存限制
) = 8 blocks
```

**第4步：Block分解为Warp**

每个Block的256个thread被分成8个warp：

```
Block_0:
  Warp_0: [Thread_0 ... Thread_31]    ← 32个thread
  Warp_1: [Thread_32 ... Thread_63]
  Warp_2: [Thread_64 ... Thread_95]
  ...
  Warp_7: [Thread_224 ... Thread_255]
```

**Warp是GPU调度的最小单位**！

**第5步：Warp调度执行**

SM内部有**4个Warp调度器**（以A100为例）：

```
Cycle 1:
  调度器0: 执行 Warp_0 (32个thread并行)
  调度器1: 执行 Warp_4 (32个thread并行)
  调度器2: 执行 Warp_8 (32个thread并行)
  调度器3: 执行 Warp_12 (32个thread并行)

Cycle 2:
  调度器0: 执行 Warp_1
  调度器1: 执行 Warp_5
  ...
```

**Warp调度的关键特性**：

1. **SIMT执行模型**：同一个warp的32个thread执行**相同的指令**

```cpp
__global__ void example(int* data) {
    int idx = threadIdx.x;
    
    // 所有32个thread同时执行同一条指令
    int val = data[idx];
    
    // Warp Divergence（分支发散）
    if (idx % 2 == 0) {
        val = val * 2;    // 16个thread执行这个分支
    } else {
        val = val + 1;    // 另外16个thread执行这个分支
    }
    // 两个分支会串行执行，性能降低！
    
    data[idx] = val;
}
```

2. **延迟隐藏**：当一个warp等待内存时，调度器切换到另一个warp

```
时间轴：
t0: Warp_0 发起内存读取（需要400 cycles）
t1: Warp_1 开始执行（隐藏Warp_0的延迟）
t2: Warp_2 开始执行
...
t400: Warp_0 数据到达，继续执行
```

这就是为什么GPU需要**大量的thread**来隐藏延迟！

#### 1.3.4 调度优化实例

**示例1：计算最优Block大小**

```cpp
// 假设要处理1,000,000个元素
int n = 1000000;

// 方案1：每个block 128个thread
int threads_1 = 128;
int blocks_1 = (n + threads_1 - 1) / threads_1;  // 7813个block
// 每个block只有4个warp，可能无法充分隐藏延迟

// 方案2：每个block 256个thread
int threads_2 = 256;
int blocks_2 = (n + threads_2 - 1) / threads_2;  // 3907个block
// 每个block有8个warp，更好的延迟隐藏

// 方案3：每个block 512个thread
int threads_3 = 512;
int blocks_3 = (n + threads_3 - 1) / threads_3;  // 1954个block
// 每个block有16个warp，但可能受资源限制

// 通常256或512是比较好的选择
```

**示例2：避免Warp Divergence**

```cpp
// 坏的做法：严重的分支发散
__global__ void bad_divergence(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 每个thread的分支都不同
        if (data[idx] > 0.0f) {
            data[idx] = sqrt(data[idx]);
        } else if (data[idx] < 0.0f) {
            data[idx] = -sqrt(-data[idx]);
        } else {
            data[idx] = 0.0f;
        }
    }
}

// 好的做法：减少分支
__global__ void good_no_divergence(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 使用数学函数避免分支
        float val = data[idx];
        float sign = copysignf(1.0f, val);
        data[idx] = sign * sqrt(fabs(val));
    }
}
```

#### 1.3.5 查看GPU调度信息

**在PyTorch中查询GPU信息**：

```python
import torch

# 获取GPU数量
num_gpus = torch.cuda.device_count()
print(f"可用GPU数量: {num_gpus}")

# 获取当前GPU的属性
device = torch.cuda.current_device()
props = torch.cuda.get_device_properties(device)

print(f"GPU名称: {props.name}")
print(f"计算能力: {props.major}.{props.minor}")
print(f"SM数量: {props.multi_processor_count}")
print(f"每个SM最大Thread数: {props.max_threads_per_multi_processor}")
print(f"每个Block最大Thread数: {props.max_threads_per_block}")
print(f"最大Grid维度: {props.max_grid_size}")
print(f"共享内存大小: {props.shared_memory_per_block / 1024} KB")
print(f"寄存器数量: {props.regs_per_block}")
print(f"Warp大小: {props.warp_size}")

# 计算理论峰值并发Thread数
max_concurrent_threads = (props.multi_processor_count * 
                         props.max_threads_per_multi_processor)
print(f"理论最大并发Thread数: {max_concurrent_threads}")
```

**使用Nsight Compute分析调度**：

```bash
# 分析kernel的调度效率
ncu --set full --export report ./my_program

# 查看占用率
ncu --metrics sm__warps_active.avg.pct_of_peak ./my_program

# 查看Warp调度效率
ncu --metrics smsp__average_warps_issue_stalled_per_issue_active \
    ./my_program
```

### 1.4 PyTorch CUDA算子架构

```mermaid
graph TB
    A[Python API] --> B[C++ Extension]
    B --> C[ATen Dispatch]
    C --> D[CUDA Kernel]
    
    D --> E[全局内存访问]
    D --> F[共享内存]
    D --> G[寄存器]
    
    E --> H[输入Tensor]
    E --> I[输出Tensor]
    
    style A fill:#e1f5ff
    style C fill:#fff4e1
    style D fill:#ffe1f5
    style G fill:#e1ffe1
```

### 1.5 开发工具链

| 工具 | 用途 | 命令 |
|------|------|------|
| nvcc | CUDA编译器 | `nvcc -arch=sm_80` |
| cuda-gdb | CUDA调试器 | `cuda-gdb ./program` |
| Nsight Compute | 性能分析 | `ncu ./program` |
| Nsight Systems | 系统级分析 | `nsys profile ./program` |
| cuda-memcheck | 内存检查 | `cuda-memcheck ./program` |

## 2 第一个CUDA算子

### 2.1 向量加法示例

**目标**：实现 `C = A + B` 的向量加法。

**CUDA Kernel实现**：

```cpp
// 文件: vector_add_kernel.cu

#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// CUDA Kernel函数
__global__ void vector_add_kernel(
    const float* __restrict__ a,
    const float* __restrict__ b,
    float* __restrict__ c,
    int n
) {
    // 计算全局线程ID
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // 边界检查
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}

// C++封装函数
torch::Tensor vector_add_cuda(
    torch::Tensor a,
    torch::Tensor b
) {
    // 检查输入
    TORCH_CHECK(a.is_cuda(), "a must be a CUDA tensor");
    TORCH_CHECK(b.is_cuda(), "b must be a CUDA tensor");
    TORCH_CHECK(a.sizes() == b.sizes(), "Size mismatch");
    
    // 创建输出张量
    auto c = torch::empty_like(a);
    
    // 获取元素数量
    int n = a.numel();
    
    // 配置kernel启动参数
    const int threads = 256;
    const int blocks = (n + threads - 1) / threads;
    
    // 启动kernel
    vector_add_kernel<<<blocks, threads>>>(
        a.data_ptr<float>(),
        b.data_ptr<float>(),
        c.data_ptr<float>(),
        n
    );
    
    // 检查错误
    cudaError_t err = cudaGetLastError();
    TORCH_CHECK(err == cudaSuccess, 
                "CUDA kernel failed: ", cudaGetErrorString(err));
    
    return c;
}

// Python绑定
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("vector_add", &vector_add_cuda, "Vector add (CUDA)");
}
```

### 2.2 编译和安装

**setup.py**：

```python
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='vector_add_cuda',
    ext_modules=[
        CUDAExtension(
            name='vector_add_cuda',
            sources=['vector_add_kernel.cu'],
            extra_compile_args={
                'cxx': ['-O3'],
                'nvcc': [
                    '-O3',
                    '--use_fast_math',
                    '-arch=sm_80',  # 根据GPU架构调整
                ]
            }
        )
    ],
    cmdclass={
        'build_ext': BuildExtension
    }
)
```

**编译命令**：

```bash
python setup.py install
```

**Python使用**：

```python
import torch
import vector_add_cuda

a = torch.randn(1000000, device='cuda')
b = torch.randn(1000000, device='cuda')

# 调用自定义CUDA算子
c = vector_add_cuda.vector_add(a, b)

# 验证正确性
assert torch.allclose(c, a + b)
```

### 2.3 线程索引计算

```mermaid
graph LR
    A[Grid] --> B[Block 0<br/>blockIdx.x=0]
    A --> C[Block 1<br/>blockIdx.x=1]
    A --> D[Block 2<br/>blockIdx.x=2]
    
    B --> E[Thread 0<br/>threadIdx.x=0]
    B --> F[Thread 1<br/>threadIdx.x=1]
    B --> G[Thread 255<br/>threadIdx.x=255]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style E fill:#e1ffe1
```

**索引计算公式**：

```cpp
// 1D索引
int idx = blockIdx.x * blockDim.x + threadIdx.x;

// 2D索引
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;

// 3D索引
int x = blockIdx.x * blockDim.x + threadIdx.x;
int y = blockIdx.y * blockDim.y + threadIdx.y;
int z = blockIdx.z * blockDim.z + threadIdx.z;
```

## 3 内存层次结构

### 3.1 GPU内存类型

```mermaid
graph TB
    A[GPU内存层次] --> B[全局内存<br/>Global Memory]
    A --> C[共享内存<br/>Shared Memory]
    A --> D[寄存器<br/>Registers]
    A --> E[常量内存<br/>Constant Memory]
    A --> F[纹理内存<br/>Texture Memory]
    
    B --> G[大小: GB级<br/>延迟: 400-800 cycles<br/>带宽: ~1TB/s]
    C --> H[大小: 48-164KB<br/>延迟: ~20 cycles<br/>带宽: ~19TB/s]
    D --> I[大小: 256KB/SM<br/>延迟: 1 cycle<br/>带宽: 最高]
    
    style A fill:#e1f5ff
    style B fill:#ffe1e1
    style C fill:#fff4e1
    style D fill:#e1ffe1
```

### 3.2 全局内存访问模式

**合并访问（Coalesced Access）**：

```cpp
// ✅ 好的访问模式：连续访问
__global__ void coalesced_access(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 线程0访问data[0]，线程1访问data[1]，...
        data[idx] = data[idx] * 2.0f;
    }
}

// ❌ 差的访问模式：跨步访问
__global__ void strided_access(float* data, int n, int stride) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 线程0访问data[0]，线程1访问data[stride]，...
        data[idx * stride] = data[idx * stride] * 2.0f;
    }
}
```

**性能对比**：
- 合并访问：~900 GB/s
- 跨步访问（stride=32）：~28 GB/s
- 性能差异：**32倍**

### 3.3 共享内存使用

**矩阵转置示例**：

```cpp
#define TILE_SIZE 32

__global__ void transpose_shared(
    const float* __restrict__ input,
    float* __restrict__ output,
    int width,
    int height
) {
    // 分配共享内存
    __shared__ float tile[TILE_SIZE][TILE_SIZE + 1];  // +1避免bank冲突
    
    // 计算全局坐标
    int x = blockIdx.x * TILE_SIZE + threadIdx.x;
    int y = blockIdx.y * TILE_SIZE + threadIdx.y;
    
    // 加载到共享内存（合并访问）
    if (x < width && y < height) {
        tile[threadIdx.y][threadIdx.x] = input[y * width + x];
    }
    
    // 同步确保所有数据加载完成
    __syncthreads();
    
    // 计算转置后的坐标
    x = blockIdx.y * TILE_SIZE + threadIdx.x;
    y = blockIdx.x * TILE_SIZE + threadIdx.y;
    
    // 从共享内存写回（合并访问）
    if (x < height && y < width) {
        output[y * height + x] = tile[threadIdx.x][threadIdx.y];
    }
}
```

**共享内存优势**：
- 延迟低：~20 cycles vs 400+ cycles
- 带宽高：~19 TB/s vs ~1 TB/s
- 线程间共享数据

### 3.4 寄存器优化

```cpp
// ❌ 未优化：重复从内存加载
__global__ void unoptimized(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        data[idx] = data[idx] * 2.0f + data[idx] * 3.0f;
        //          ^^^^^^^^          ^^^^^^^^
        //          两次内存访问
    }
}

// ✅ 优化：使用寄存器缓存
__global__ void optimized(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float val = data[idx];  // 一次内存读取
        data[idx] = val * 2.0f + val * 3.0f;  // 从寄存器读取
    }
}
```

## 4 PyTorch CUDA算子集成

### 4.1 ATen张量操作

```cpp
#include <ATen/ATen.h>
#include <ATen/cuda/CUDAContext.h>

__global__ void relu_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        output[idx] = fmaxf(input[idx], 0.0f);
    }
}

at::Tensor relu_cuda(const at::Tensor& input) {
    // 检查输入
    TORCH_CHECK(input.is_cuda(), "Input must be a CUDA tensor");
    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");
    
    // 创建输出张量
    auto output = at::empty_like(input);
    
    // 获取元素数量
    int n = input.numel();
    
    // Kernel配置
    const int threads = 256;
    const int blocks = (n + threads - 1) / threads;
    
    // 获取CUDA stream
    auto stream = at::cuda::getCurrentCUDAStream();
    
    // 启动kernel
    relu_kernel<<<blocks, threads, 0, stream>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        n
    );
    
    return output;
}
```

### 4.2 多维张量处理

```cpp
// 2D卷积示例
__global__ void conv2d_kernel(
    const float* __restrict__ input,    // [N, C, H, W]
    const float* __restrict__ weight,   // [K, C, R, S]
    float* __restrict__ output,         // [N, K, H', W']
    int N, int C, int H, int W,
    int K, int R, int S,
    int H_out, int W_out
) {
    // 输出位置
    int n = blockIdx.z;
    int k = blockIdx.y;
    int h_out = blockIdx.x * blockDim.x + threadIdx.x;
    int w_out = threadIdx.y;
    
    if (h_out >= H_out || w_out >= W_out) return;
    
    float sum = 0.0f;
    
    // 卷积计算
    for (int c = 0; c < C; c++) {
        for (int r = 0; r < R; r++) {
            for (int s = 0; s < S; s++) {
                int h_in = h_out + r;
                int w_in = w_out + s;
                
                if (h_in < H && w_in < W) {
                    int input_idx = ((n * C + c) * H + h_in) * W + w_in;
                    int weight_idx = ((k * C + c) * R + r) * S + s;
                    
                    sum += input[input_idx] * weight[weight_idx];
                }
            }
        }
    }
    
    // 写入输出
    int output_idx = ((n * K + k) * H_out + h_out) * W_out + w_out;
    output[output_idx] = sum;
}

at::Tensor conv2d_cuda(
    const at::Tensor& input,
    const at::Tensor& weight
) {
    auto N = input.size(0);
    auto C = input.size(1);
    auto H = input.size(2);
    auto W = input.size(3);
    
    auto K = weight.size(0);
    auto R = weight.size(2);
    auto S = weight.size(3);
    
    auto H_out = H - R + 1;
    auto W_out = W - S + 1;
    
    // 创建输出
    auto output = at::empty({N, K, H_out, W_out}, input.options());
    
    // Kernel配置
    dim3 threads(16, 16);
    dim3 blocks(
        (H_out + threads.x - 1) / threads.x,
        K,
        N
    );
    
    // 启动kernel
    conv2d_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        weight.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W, K, R, S, H_out, W_out
    );
    
    return output;
}
```

### 4.3 Autograd集成

```cpp
#include <torch/extension.h>

// 前向kernel
__global__ void sigmoid_forward_kernel(
    const float* __restrict__ x,
    float* __restrict__ y,
    int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        y[idx] = 1.0f / (1.0f + expf(-x[idx]));
    }
}

// 反向kernel
__global__ void sigmoid_backward_kernel(
    const float* __restrict__ grad_output,
    const float* __restrict__ output,
    float* __restrict__ grad_input,
    int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float y = output[idx];
        grad_input[idx] = grad_output[idx] * y * (1.0f - y);
    }
}

// Autograd Function
class SigmoidCUDA : public torch::autograd::Function<SigmoidCUDA> {
public:
    static torch::Tensor forward(
        torch::autograd::AutogradContext* ctx,
        torch::Tensor input
    ) {
        auto output = torch::empty_like(input);
        
        int n = input.numel();
        const int threads = 256;
        const int blocks = (n + threads - 1) / threads;
        
        sigmoid_forward_kernel<<<blocks, threads>>>(
            input.data_ptr<float>(),
            output.data_ptr<float>(),
            n
        );
        
        // 保存输出用于反向传播
        ctx->save_for_backward({output});
        
        return output;
    }
    
    static torch::autograd::tensor_list backward(
        torch::autograd::AutogradContext* ctx,
        torch::autograd::tensor_list grad_outputs
    ) {
        auto saved = ctx->get_saved_variables();
        auto output = saved[0];
        auto grad_output = grad_outputs[0];
        
        auto grad_input = torch::empty_like(grad_output);
        
        int n = grad_output.numel();
        const int threads = 256;
        const int blocks = (n + threads - 1) / threads;
        
        sigmoid_backward_kernel<<<blocks, threads>>>(
            grad_output.data_ptr<float>(),
            output.data_ptr<float>(),
            grad_input.data_ptr<float>(),
            n
        );
        
        return {grad_input};
    }
};

// Python接口
torch::Tensor sigmoid_cuda(torch::Tensor input) {
    return SigmoidCUDA::apply(input);
}
```

## 5 Kernel启动优化

### 5.1 最优线程块配置

```cpp
// 计算最优线程块大小
int getOptimalBlockSize(int n, int max_threads = 1024) {
    // 常用的线程块大小
    const int candidates[] = {32, 64, 128, 256, 512, 1024};
    
    // 选择能最大化占用率的大小
    int best_size = 256;  // 默认值
    float best_occupancy = 0.0f;
    
    for (int size : candidates) {
        if (size > max_threads) continue;
        
        int blocks_needed = (n + size - 1) / size;
        float occupancy = calculateOccupancy(size, blocks_needed);
        
        if (occupancy > best_occupancy) {
            best_occupancy = occupancy;
            best_size = size;
        }
    }
    
    return best_size;
}

// 使用示例
void launch_kernel(const float* input, float* output, int n) {
    int threads = getOptimalBlockSize(n);
    int blocks = (n + threads - 1) / threads;
    
    my_kernel<<<blocks, threads>>>(input, output, n);
}
```

### 5.2 占用率计算

**占用率（Occupancy）**：实际活跃warp数 / 最大可能warp数

```cpp
#include <cuda_occupancy.h>

void analyze_kernel_occupancy() {
    int blockSize;
    int minGridSize;
    int gridSize;
    
    // 自动计算最优配置
    cudaOccupancyMaxPotentialBlockSize(
        &minGridSize,
        &blockSize,
        my_kernel,
        0,  // 动态共享内存大小
        0   // 每个块的最大线程数
    );
    
    printf("建议线程块大小: %d\n", blockSize);
    printf("建议网格大小: %d\n", minGridSize);
    
    // 计算实际占用率
    int maxActiveBlocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        &maxActiveBlocks,
        my_kernel,
        blockSize,
        0
    );
    
    int device;
    cudaDeviceProp props;
    cudaGetDevice(&device);
    cudaGetDeviceProperties(&props, device);
    
    float occupancy = (maxActiveBlocks * blockSize / 
                      (float)props.maxThreadsPerMultiProcessor) * 100;
    
    printf("占用率: %.2f%%\n", occupancy);
}
```

### 5.3 Grid-Stride循环

```cpp
// Grid-Stride循环模式
__global__ void grid_stride_kernel(float* data, int n) {
    // 每个线程处理多个元素
    for (int idx = blockIdx.x * blockDim.x + threadIdx.x;
         idx < n;
         idx += blockDim.x * gridDim.x) {
        
        data[idx] = data[idx] * 2.0f;
    }
}

// 启动少量线程块，每个处理多个元素
void launch_grid_stride(float* data, int n) {
    int threads = 256;
    int blocks = min(32 * 80, (n + threads - 1) / threads);  // 限制块数
    
    grid_stride_kernel<<<blocks, threads>>>(data, n);
}
```

**优势**：
- 减少kernel启动开销
- 更好的指令缓存利用
- 适应不同大小的输入

## 6 错误处理和调试

### 6.1 CUDA错误检查

```cpp
// 错误检查宏
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error in %s:%d: %s\n", \
                    __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// Kernel错误检查
#define CUDA_KERNEL_CHECK() \
    do { \
        cudaError_t err = cudaGetLastError(); \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA kernel error: %s\n", \
                    cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
        err = cudaDeviceSynchronize(); \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA sync error: %s\n", \
                    cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// 使用示例
CUDA_CHECK(cudaMalloc(&d_data, size));
my_kernel<<<blocks, threads>>>(d_data, n);
CUDA_KERNEL_CHECK();
```

### 6.2 Kernel内部调试

```cpp
// 使用printf调试
__global__ void debug_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        // 只在特定线程打印
        if (idx == 0 || idx == n - 1) {
            printf("Thread %d: data[%d] = %f\n", idx, idx, data[idx]);
        }
        
        data[idx] = data[idx] * 2.0f;
    }
}

// 使用assert断言
__global__ void assert_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        // 运行时断言
        assert(data[idx] >= 0.0f && "Data must be non-negative");
        
        data[idx] = sqrtf(data[idx]);
    }
}
```

### 6.3 内存越界检查

```cpp
// 使用cuda-memcheck检测内存错误
// 命令: cuda-memcheck ./program

__global__ void potential_bug(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // 可能越界访问
    data[idx] = data[idx] * 2.0f;
    
    // 加边界检查
    if (idx < n) {
        data[idx] = data[idx] * 2.0f;
    }
}
```

### 6.4 竞态条件检测

```cpp
// cuda-racecheck检测竞态条件
// 命令: cuda-racecheck ./program

__global__ void race_condition_example(int* counter) {
    // 竞态条件：多个线程同时写
    *counter = *counter + 1;
    
    // 使用原子操作
    atomicAdd(counter, 1);
}
```

## 7 性能优化基础

### 7.1 内存访问优化

```cpp
// 案例：矩阵乘法优化

// 未优化版本
__global__ void matmul_naive(
    const float* A, const float* B, float* C,
    int M, int N, int K
) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[row * K + k] * B[k * N + col];
            //     ^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^
            //     非合并访问         非合并访问
        }
        C[row * N + col] = sum;
    }
}

// 使用共享内存优化
#define TILE_SIZE 16

__global__ void matmul_shared(
    const float* A, const float* B, float* C,
    int M, int N, int K
) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];
    
    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;
    
    float sum = 0.0f;
    
    // 分块计算
    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {
        // 加载A的tile到共享内存
        if (row < M && t * TILE_SIZE + threadIdx.x < K) {
            As[threadIdx.y][threadIdx.x] = 
                A[row * K + t * TILE_SIZE + threadIdx.x];
        } else {
            As[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        // 加载B的tile到共享内存
        if (col < N && t * TILE_SIZE + threadIdx.y < K) {
            Bs[threadIdx.y][threadIdx.x] = 
                B[(t * TILE_SIZE + threadIdx.y) * N + col];
        } else {
            Bs[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        __syncthreads();
        
        // 计算部分乘积
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        
        __syncthreads();
    }
    
    // 写回结果
    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}
```

**性能对比**：
- naive版本：~100 GFlops
- shared版本：~800 GFlops
- cuBLAS：~8000 GFlops

### 7.2 循环展开

```cpp
// 手动循环展开
__global__ void reduce_unrolled(float* input, float* output, int n) {
    __shared__ float sdata[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;
    
    // 加载并展开4个元素
    float sum = 0.0f;
    if (idx < n) sum += input[idx];
    if (idx + blockDim.x < n) sum += input[idx + blockDim.x];
    if (idx + 2 * blockDim.x < n) sum += input[idx + 2 * blockDim.x];
    if (idx + 3 * blockDim.x < n) sum += input[idx + 3 * blockDim.x];
    
    sdata[tid] = sum;
    __syncthreads();
    
    // 归约（展开最后几步）
    if (blockDim.x >= 512) {
        if (tid < 256) sdata[tid] += sdata[tid + 256];
        __syncthreads();
    }
    if (blockDim.x >= 256) {
        if (tid < 128) sdata[tid] += sdata[tid + 128];
        __syncthreads();
    }
    if (blockDim.x >= 128) {
        if (tid < 64) sdata[tid] += sdata[tid + 64];
        __syncthreads();
    }
    
    // Warp级归约（无需同步）
    if (tid < 32) {
        volatile float* smem = sdata;
        smem[tid] += smem[tid + 32];
        smem[tid] += smem[tid + 16];
        smem[tid] += smem[tid + 8];
        smem[tid] += smem[tid + 4];
        smem[tid] += smem[tid + 2];
        smem[tid] += smem[tid + 1];
    }
    
    if (tid == 0) output[blockIdx.x] = sdata[0];
}
```

### 7.3 指令级优化

```cpp
// 使用快速数学函数
__global__ void fast_math_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float x = data[idx];
        
        // 使用快速近似函数
        float y = __fdividef(1.0f, x);           // 快速除法
        float z = __expf(x);                     // 快速exp
        float w = rsqrtf(x);                     // 快速1/sqrt
        
        data[idx] = y + z + w;
    }
}

// 编译选项优化
// nvcc -O3 --use_fast_math -arch=sm_80 kernel.cu
```

## 8 实战案例

### 8.1 Softmax实现

```cpp
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

// Softmax kernel
__global__ void softmax_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int batch_size,
    int dim
) {
    int b = blockIdx.x;
    
    if (b >= batch_size) return;
    
    const float* input_row = input + b * dim;
    float* output_row = output + b * dim;
    
    // 1. 找最大值（数值稳定性）
    float max_val = -INFINITY;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        max_val = fmaxf(max_val, input_row[i]);
    }
    
    // 归约找全局最大值
    __shared__ float shared_max[256];
    shared_max[threadIdx.x] = max_val;
    __syncthreads();
    
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_max[threadIdx.x] = fmaxf(
                shared_max[threadIdx.x],
                shared_max[threadIdx.x + stride]
            );
        }
        __syncthreads();
    }
    max_val = shared_max[0];
    
    // 2. 计算exp(x - max)并求和
    float sum = 0.0f;
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        float exp_val = expf(input_row[i] - max_val);
        output_row[i] = exp_val;
        sum += exp_val;
    }
    
    // 归约求和
    __shared__ float shared_sum[256];
    shared_sum[threadIdx.x] = sum;
    __syncthreads();
    
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
        }
        __syncthreads();
    }
    sum = shared_sum[0];
    
    // 3. 归一化
    for (int i = threadIdx.x; i < dim; i += blockDim.x) {
        output_row[i] /= sum;
    }
}

torch::Tensor softmax_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_cuda(), "Input must be CUDA tensor");
    TORCH_CHECK(input.dim() == 2, "Input must be 2D");
    
    auto batch_size = input.size(0);
    auto dim = input.size(1);
    
    auto output = torch::empty_like(input);
    
    const int threads = 256;
    const int blocks = batch_size;
    
    softmax_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        batch_size,
        dim
    );
    
    return output;
}
```

### 8.2 LayerNorm实现

```cpp
__global__ void layer_norm_kernel(
    const float* __restrict__ input,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    float* __restrict__ output,
    int batch_size,
    int hidden_size,
    float eps
) {
    int b = blockIdx.x;
    if (b >= batch_size) return;
    
    const float* input_row = input + b * hidden_size;
    float* output_row = output + b * hidden_size;
    
    // 1. 计算均值
    float sum = 0.0f;
    for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
        sum += input_row[i];
    }
    
    __shared__ float shared_sum[256];
    shared_sum[threadIdx.x] = sum;
    __syncthreads();
    
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
        }
        __syncthreads();
    }
    float mean = shared_sum[0] / hidden_size;
    
    // 2. 计算方差
    float var_sum = 0.0f;
    for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
        float diff = input_row[i] - mean;
        var_sum += diff * diff;
    }
    
    shared_sum[threadIdx.x] = var_sum;
    __syncthreads();
    
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
        }
        __syncthreads();
    }
    float variance = shared_sum[0] / hidden_size;
    float inv_std = rsqrtf(variance + eps);
    
    // 3. 归一化并应用仿射变换
    for (int i = threadIdx.x; i < hidden_size; i += blockDim.x) {
        float normalized = (input_row[i] - mean) * inv_std;
        output_row[i] = gamma[i] * normalized + beta[i];
    }
}
```

### 8.3 性能测试

```python
import torch
import time

def benchmark_kernel(func, *args, warmup=10, iterations=100):
    # 预热
    for _ in range(warmup):
        func(*args)
    
    torch.cuda.synchronize()
    
    # 测试
    start = time.time()
    for _ in range(iterations):
        func(*args)
    torch.cuda.synchronize()
    
    elapsed = (time.time() - start) / iterations
    return elapsed * 1000  # 转换为毫秒

# 测试softmax
batch_size = 128
dim = 1024

x = torch.randn(batch_size, dim, device='cuda')

# PyTorch实现
time_pytorch = benchmark_kernel(torch.softmax, x, dim=1)
print(f"PyTorch Softmax: {time_pytorch:.3f} ms")

# 自定义CUDA实现
import softmax_cuda
time_custom = benchmark_kernel(softmax_cuda.forward, x)
print(f"Custom CUDA Softmax: {time_custom:.3f} ms")

# 验证正确性
y_pytorch = torch.softmax(x, dim=1)
y_custom = softmax_cuda.forward(x)
print(f"最大误差: {(y_pytorch - y_custom).abs().max().item():.6f}")
```

## 9 最佳实践

### 9.1 开发流程

```mermaid
graph TB
    A[算法设计] --> B[CPU参考实现]
    B --> C[CUDA naive实现]
    C --> D[正确性验证]
    D --> E[性能分析]
    E --> F[优化迭代]
    F --> G{性能目标}
    G -->|未达标| E
    G -->|达标| H[集成到PyTorch]
    
    style A fill:#e1f5ff
    style D fill:#fff4e1
    style E fill:#ffe1f5
    style H fill:#e1ffe1
```

### 9.2 优化清单

- [ ] 使用合并内存访问
- [ ] 利用共享内存减少全局内存访问
- [ ] 最大化占用率
- [ ] 循环展开减少分支
- [ ] 使用快速数学函数
- [ ] 避免warp divergence
- [ ] 使用Grid-Stride循环
- [ ] Profile识别瓶颈
- [ ] 验证数值精度

### 9.3 常见陷阱

```cpp
// 错误1：忘记同步
__global__ void bug_no_sync() {
    __shared__ float data[256];
    data[threadIdx.x] = threadIdx.x;
    // 缺少__syncthreads()
    float val = data[(threadIdx.x + 1) % 256];  // 可能读到未初始化的值
}

// 错误2：Bank冲突
__shared__ float bad_access[32][32];
bad_access[threadIdx.x][0] = 1.0f;  // 所有线程访问同一bank

// 修正：添加padding
__shared__ float good_access[32][33];  // +1避免冲突

// 错误3：寄存器溢出
__global__ void too_many_registers() {
    float temp[1000];  // 太多局部变量，溢出到本地内存
    // ...
}
```

## 10 总结

### 10.1 关键要点

1. **内存层次**：寄存器 > 共享内存 > 全局内存
2. **访问模式**：合并访问性能最优
3. **占用率**：平衡寄存器、共享内存使用
4. **同步开销**：最小化`__syncthreads()`
5. **Warp效率**：避免分支divergence

### 10.2 性能优化路线

```mermaid
graph LR
    A[Naive实现] --> B[合并访问<br/>2-10x]
    B --> C[共享内存<br/>2-5x]
    C --> D[循环展开<br/>1.2-2x]
    D --> E[指令优化<br/>1.1-1.5x]
    E --> F[Tensor Core<br/>5-10x]
    
    style A fill:#ffe1e1
    style C fill:#fff4e1
    style F fill:#e1ffe1
```

---